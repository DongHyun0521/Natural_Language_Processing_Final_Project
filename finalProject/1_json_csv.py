{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP1LD2VVxHetQU3Xh59WrfI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#json_csv.py"],"metadata":{"id":"ODrxRiV2ihk0"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import os"],"metadata":{"id":"PHiyBQRRilYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGp6uL7aiZXg"},"outputs":[],"source":["# Colab에서 py 파일과 같은 위치라면, 현재 디렉토리 사용\n","base_dir = '.'  # 또는 '/content' (Colab의 기본 작업 디렉토리)\n","\n","all_json_files = []\n","for i in range(1, 6):\n","    json_path = os.path.join(base_dir, f\"talksets-train-{i}_aihub.json\")\n","    if os.path.isfile(json_path):\n","        print(\"발견:\", json_path)\n","        all_json_files.append(json_path)\n","\n","print(\"발견된 파일:\", all_json_files)\n","\n","all_data = []\n","for json_file in all_json_files:\n","    with open(json_file, encoding='utf-8') as f:\n","        conversations = json.load(f)\n","        for conv in conversations:\n","            for sent in conv['sentences']:\n","                text = sent.get('origin_text', sent.get('text'))\n","                label = 1 if sent['is_immoral'] else 0\n","                all_data.append({'text': text, 'label': label})\n","\n","df = pd.DataFrame(all_data)\n","df = df.drop_duplicates()\n","save_path = os.path.join(base_dir, \"hate_speech_dataset_1.csv\")\n","df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"완료! 총 {len(df)}개의 샘플을 hate_speech_dataset.csv에 저장했습니다.\")\n","if len(df) >= 5:\n","    print(df.sample(5))\n","else:\n","    print(df)"]}]}